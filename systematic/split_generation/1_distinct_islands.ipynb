{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anonymous/miniconda3/envs/belka/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow.compute as pc\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ParquetSchema object at 0x72ae81e65b80>\n",
      "required group field_id=-1 schema {\n",
      "  optional int64 field_id=-1 id;\n",
      "  optional binary field_id=-1 buildingblock1_smiles (String);\n",
      "  optional binary field_id=-1 buildingblock2_smiles (String);\n",
      "  optional binary field_id=-1 buildingblock3_smiles (String);\n",
      "  optional binary field_id=-1 molecule_smiles (String);\n",
      "  optional binary field_id=-1 protein_name (String);\n",
      "  optional int64 field_id=-1 binds;\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Optionally, you can inspect the schema to confirm field names and types\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(parquet_file\u001b[38;5;241m.\u001b[39mschema)\n\u001b[0;32m----> 6\u001b[0m column_data \u001b[38;5;241m=\u001b[39m \u001b[43mparquet_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbuildingblock1_smiles\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m unique_values \u001b[38;5;241m=\u001b[39m pc\u001b[38;5;241m.\u001b[39munique(column_data\u001b[38;5;241m.\u001b[39mcolumn(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      8\u001b[0m unique_list1_train \u001b[38;5;241m=\u001b[39m unique_values\u001b[38;5;241m.\u001b[39mto_pylist()\n",
      "File \u001b[0;32m~/miniconda3/envs/belka/lib/python3.10/site-packages/pyarrow/parquet/core.py:623\u001b[0m, in \u001b[0;36mParquetFile.read\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mRead a Table from Parquet format.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03manimal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m column_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_column_indices(\n\u001b[1;32m    622\u001b[0m     columns, use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[0;32m--> 623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile('../../train.parquet')\n",
    "total_records = parquet_file.metadata.num_rows\n",
    "# Optionally, you can inspect the schema to confirm field names and types\n",
    "print(parquet_file.schema)\n",
    "\n",
    "column_data = parquet_file.read(columns=['buildingblock1_smiles'])\n",
    "unique_values = pc.unique(column_data.column(0))\n",
    "unique_list1_train = unique_values.to_pylist()\n",
    "\n",
    "column_data = parquet_file.read(columns=['buildingblock2_smiles'])\n",
    "unique_values = pc.unique(column_data.column(0))\n",
    "unique_list2_train = unique_values.to_pylist()\n",
    "\n",
    "column_data = parquet_file.read(columns=['buildingblock3_smiles'])\n",
    "unique_values = pc.unique(column_data.column(0))\n",
    "unique_list3_train = unique_values.to_pylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../spl1.pkl','wb') as f:\n",
    "    pickle.dump(unique_list1_train,f)\n",
    "with open('../spl2.pkl','wb') as f:\n",
    "    pickle.dump(unique_list2_train,f)\n",
    "with open('../spl3.pkl','wb') as f:\n",
    "    pickle.dump(unique_list3_train,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = list(set(unique_list2_train).union(set(unique_list3_train).union(set(unique_list1_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_to_i = {x: i for i, x in enumerate(blocks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.ParquetFile('../../test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split(x):\n",
    "    x = x.to_pandas()\n",
    "    graph = np.zeros((2110,2110), dtype=bool)\n",
    "    \n",
    "    for k, v in x.iterrows():       \n",
    "        graph[blocks_to_i[v['buildingblock1_smiles']],blocks_to_i[v['buildingblock1_smiles']]] = True\n",
    "        graph[blocks_to_i[v['buildingblock2_smiles']],blocks_to_i[v['buildingblock2_smiles']]] = True\n",
    "        graph[blocks_to_i[v['buildingblock3_smiles']],blocks_to_i[v['buildingblock3_smiles']]] = True\n",
    "\n",
    "        graph[blocks_to_i[v['buildingblock1_smiles']],blocks_to_i[v['buildingblock2_smiles']]] = True\n",
    "        graph[blocks_to_i[v['buildingblock2_smiles']],blocks_to_i[v['buildingblock1_smiles']]] = True\n",
    "\n",
    "        graph[blocks_to_i[v['buildingblock1_smiles']],blocks_to_i[v['buildingblock3_smiles']]] = True\n",
    "        graph[blocks_to_i[v['buildingblock3_smiles']],blocks_to_i[v['buildingblock1_smiles']]] = True\n",
    "\n",
    "        graph[blocks_to_i[v['buildingblock2_smiles']],blocks_to_i[v['buildingblock3_smiles']]] = True\n",
    "        graph[blocks_to_i[v['buildingblock3_smiles']],blocks_to_i[v['buildingblock2_smiles']]] = True\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 17/17 [00:10<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH = 100_000\n",
    "total_records = parquet_file.metadata.num_rows\n",
    "total_batches = (total_records + BATCH - 1) // BATCH  # Ceiling division  \n",
    "with Pool(processes=30) as pool:  # Adjust the number of processes according to your system\n",
    "        # Use imap_unordered to process data as it is read\n",
    "        results = list(tqdm(pool.imap_unordered(process_split, parquet_file.iter_batches(batch_size=BATCH)),\n",
    "                            total=total_batches, desc=\"Processing batches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stack = np.stack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2110, 2110)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = results_stack.any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_graph = csr_matrix(final.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of connected components: 4\n"
     ]
    }
   ],
   "source": [
    "n_components, labels = connected_components(csgraph=sparse_graph, directed=False, return_labels=True)\n",
    "\n",
    "print(f\"Number of connected components: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3], dtype=int32), array([ 859, 1145,   53,   53]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MAIN = '/home/anonymous/belka/test.parquet'\n",
    "test = pd.read_parquet(TEST_MAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(row):\n",
    "    return labels[blocks_to_i[row['buildingblock1_smiles']]]\n",
    "test['island'] = test.apply(get_score, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_parquet('/home/anonymous/belka/test_with_islands.parquet',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533813"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((test['island']==) == ((test['buildingblock3_smiles'].isin(set(blocks))) | (test['buildingblock2_smiles'].isin(set(blocks))) | (test['buildingblock1_smiles'].isin(set(blocks)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           True\n",
       "1           True\n",
       "2           True\n",
       "3           True\n",
       "4           True\n",
       "           ...  \n",
       "1674891    False\n",
       "1674892    False\n",
       "1674893    False\n",
       "1674894    False\n",
       "1674895    False\n",
       "Length: 1674896, dtype: bool"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('test_connected.pickle','wb') as f:\n",
    "#     pickle.dump((labels, np.unique(labels, return_counts=True), blocks_to_i), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph(sparse_graph)\n",
    "cut_value, (set1, set2) = nx.stoer_wagner(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
