{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['belka']  # Replace with your database name\n",
    "collection = db['train_metadata']  # Replace with your collection name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 242765358/290000000 [17:05<03:19, 236827.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query for filtering documents where 'is_test' is True and retrieving only 'id'\n",
    "query = {\"test_u\": False, \"test1\": False, \"test2\": False}\n",
    "projection = {\"_id\": 0, \"id\": 1}  # '_id' is included by default, explicitly exclude it\n",
    "\n",
    "# Prepare to fetch documents in batches\n",
    "cursor = collection.find(query, projection).batch_size(10000)  # Adjust the batch size as needed\n",
    "\n",
    "# Retrieve and store ids\n",
    "ids = [int(doc['id']) for doc in tqdm(cursor, total=290000000)]\n",
    "\n",
    "# Serialize and save the list to a file using pickle\n",
    "with open('ids_train.pkl', 'wb') as f:\n",
    "    pickle.dump(ids, f)\n",
    "\n",
    "print(\"Data has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "with open('ids_train.msgpack', 'wb') as f:\n",
    "    packed = msgpack.packb(ids, use_bin_type=True)\n",
    "    f.write(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31800945it [02:36, 202580.56it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query for filtering documents where 'is_test' is True and retrieving only 'id'\n",
    "query = {\"test_u\": True}\n",
    "projection = {\"_id\": 0, \"id\": 1}  # '_id' is included by default, explicitly exclude it\n",
    "\n",
    "# Prepare to fetch documents in batches\n",
    "cursor = collection.find(query, projection).batch_size(10000)  # Adjust the batch size as needed\n",
    "\n",
    "# Retrieve and store ids\n",
    "ids = [int(doc['id']) for doc in tqdm(cursor, total=29000000)]\n",
    "\n",
    "# Serialize and save the list to a file using pickle\n",
    "with open('ids_test_u.msgpack', 'wb') as f:\n",
    "    packed = msgpack.packb(ids, use_bin_type=True)\n",
    "    f.write(packed)\n",
    "    \n",
    "print(\"Data has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 8857748/29000000 [01:11<02:41, 124632.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query for filtering documents where 'is_test' is True and retrieving only 'id'\n",
    "query = {\"test1\": True}\n",
    "projection = {\"_id\": 0, \"id\": 1}  # '_id' is included by default, explicitly exclude it\n",
    "\n",
    "# Prepare to fetch documents in batches\n",
    "cursor = collection.find(query, projection).batch_size(10000)  # Adjust the batch size as needed\n",
    "\n",
    "# Retrieve and store ids\n",
    "ids = [int(doc['id']) for doc in tqdm(cursor, total=29000000)]\n",
    "\n",
    "# Serialize and save the list to a file using pickle\n",
    "with open('ids_test1.msgpack', 'wb') as f:\n",
    "    packed = msgpack.packb(ids, use_bin_type=True)\n",
    "    f.write(packed)\n",
    "print(\"Data has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295246830it [12:58, 379481.31it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query for filtering documents where 'is_test' is True and retrieving only 'id'\n",
    "query = {}\n",
    "projection = {\"_id\": 0, \"id\": 1}  # '_id' is included by default, explicitly exclude it\n",
    "\n",
    "# Prepare to fetch documents in batches\n",
    "cursor = collection.find(query, projection).batch_size(10000)  # Adjust the batch size as needed\n",
    "\n",
    "# Retrieve and store ids\n",
    "ids = [int(doc['id']) for doc in tqdm(cursor, total=29000000)]\n",
    "\n",
    "# Serialize and save the list to a file using pickle\n",
    "with open('ids_all.msgpack', 'wb') as f:\n",
    "    packed = msgpack.packb(ids, use_bin_type=True)\n",
    "    f.write(packed)\n",
    "\n",
    "print(\"Data has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db['test_metadata']  # Replace with your collection name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1674896/29000000 [00:04<01:11, 379836.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved.\n"
     ]
    }
   ],
   "source": [
    "import msgpack\n",
    "# Query for filtering documents where 'is_test' is True and retrieving only 'id'\n",
    "query = {}\n",
    "projection = {\"_id\": 0, \"id\": 1}  # '_id' is included by default, explicitly exclude it\n",
    "\n",
    "# Prepare to fetch documents in batches\n",
    "cursor = collection.find(query, projection).batch_size(10000)  # Adjust the batch size as needed\n",
    "\n",
    "# Retrieve and store ids\n",
    "ids = [int(doc['id']) for doc in tqdm(cursor, total=29000000)]\n",
    "\n",
    "# Serialize and save the list to a file using pickle\n",
    "with open('ids_test.msgpack', 'wb') as f:\n",
    "    packed = msgpack.packb(ids, use_bin_type=True)\n",
    "    f.write(packed)\n",
    "\n",
    "print(\"Data has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674896"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
